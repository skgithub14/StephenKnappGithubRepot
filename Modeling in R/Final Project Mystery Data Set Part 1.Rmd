---
title: 'CSCI E-63C: Final Exam/Project'
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Preface

For the final exam/project we will develop classification models using several approaches and compare their performance on a new dataset that is a subset of one of the datasets used in machine learning common task framework (CTF) competitions.  A copy of it split into training (`final-data-train.csv`, with outcome `response` available) and test (`final-data-test.csv`, stripped of the outcome, for prediction purposes only) datasets is available on our course website in Canvas as a zip-archive of all associated files.

Please notice, that at the end of this final exam/project you will be asked, in addition to the Rmarkdown and HTML files, to also make predictions for the observations in the *test* (not training!) dataset and upload them into Canvas as well.  The expected format for the file with predictions for test dataset is two columns of comma-separated values, one row per observation in *test* dataset, first column -- the observation identifier (column `id` in test dataset) and the second column -- your best model predictions for each observation in the *test* dataset as Y/N indicator.  To illustrate expected format the zip archive contains also a couple of examples of test predictions in this format for your reference as well (`predictions-*.csv` files in `predictions-examples` sub-folder in zip-archive).

One more time, to iterate and emphasize, please notice that this time your submission must consist of the following *three* (not just two, Rmd+html, as usual) items:

* Rmarkdown *.Rmd file with all the calculations you want to receive credit for,
* HTML version of the output generated by your *.Rmd file, and
* **predictions** for the **test** dataset in comma-separated values (CSV) format (file name **must** have *.csv extension for the file to load in Canvas)

The teaching team invites you to load your predictions (just predictions, just for the test dataset according to the file format shown in the sample files in the zip-archive) into Canvas repeatedly as you work on your models and improve them over the course of this week.  At least daily (or more frequently as we see fit) we will download predictions loaded by everyone by that time and compile a leaderboard in html format of all of them sorted by their accuracy as compared to the true values of the outcome for the test dataset (along with their sensitivity, specificity, etc.).  This list will be made available on our course website in Canvas for everyone in this class in order to see how the performance of their models compares across the rest of the models built by other students in the class.  The first version of the leaderboard posted on the course website at the time when final exam is made available starts with predictions made by those few example files provided in the zip-archive (coin flip, majority vote, etc. -- what do you think Charlie Brown is using to make the predictions?).  Those should be pretty easy to improve upon.

It is 100% up to you whether you want to upload your model predictions over the course of this week, how frequently you want to do it and what you want its results to be called in the leaderboard posted for everyone in the class to see.  We will use the name of the 2nd column in the file with predictions (the one containing Y/N values, not the numerical ids of the observations) as the model name listed in the leaderboard.  If you prefer not to use your name, choose something else instead, sufficiently unique so that it is easier for you to spot your result among all others.  Once again, please check out sample files of dull (majority vote, coin flip, etc.) predictions we have made available and consider how they show up in the leaderboard html file already posted on Canvas website.  Once you are done with final you are expected to load predictions from your best model into Canvas -- that is part of your points total as explained below.

Lastly, the size of this dataset can make some of the modeling techniques run slower than what we were typically encountering in this class.  You may find it helpful to do some of the exploration and model tuning on multiple random samples of smaller size as you decide on useful ranges of parameters/modeling choices, and then only perform a final run of fully debugged and working code on the full dataset.  Please see also the afterword below on the computational demands of this problem set.

# Problem 1: univariate and unsupervised analysis (20 points)

Download and read training and test data into R and prepare graphical and numerical summaries of it: e.g. histograms of continuous attributes, contingency tables of categorical variables, scatterplots of continuous attributes with some of the categorical variables indicated by color/symbol shape, etc.  Whatever you find helpful to think about properties of the data you are about to start using for fitting classification models.

As it is often the case for such contests, the attributes in the dataset are blinded in the sense that no information is available about what those are or what their values mean.  The only information available is that the attribute `response` is the outcome to be modeled and the attribute `id` is the unique numerical identifier for each observation.  Some of the remaining attributes are clearly categorical (those that are character valued) and some rather obviously continuous (those with numerical values with large number of unique values).  For several of them it is less clear whether it is best to treat them as continuous or categorical -- e.g. their values are numerical but there are relatively few unique values with many observations taking the same value, so that they arguably could be treated as continuous or categorical.  Please idenify them, reflect on how you prefer to handle them and describe this in your own words.

Perform principal components analysis of this data (do you need to scale it prior to that? how would you represent multilevel categorical attributes to be used as inputs for PCA?) and plot observations in the space of the first few principal components indicating levels of some of the categorical attributes of your choosing by the color/shape of the symbol.  Perform univariate assessment of associations between the outcome we will be modeling and each of the attributes (e.g. t-test or logistic regression for continuous attributes, contingency tables/Fisher exact test/$\chi^2$ test for categorical attributes).  Summarize your observations from these assessments: does it appear that there are predictors associated with the outcome `response` univariately? Which predictors seem to be more/less relevant?

## Load Libraries and Raw Data

```{r}
library(ggplot2)
library(GGally)
library(gridExtra)
library(MASS)
library(glmnet)
library(randomForest)
library(reshape2)
```

```{r}
# read in the raw training data
raw.data <- read.csv("final-data-train.csv")
head(raw.data)
```

## Data Exploration

```{r}
# understanding the shape of the data frame
cat("Dims:", dim(raw.data))
cat("\n\n\n")
str(raw.data)
cat("\n\n\n")
summary(raw.data)
cat("\n\n\n")
#check for NA values
sapply(raw.data, function(x) sum(is.na(x)))
```

Raw contingency tables:

```{r}
# discrete variable contingency tables
discrete.list <- list(
    "wc",
    "zwp",
    "wi",
    "bnf",
    "ypz",
    "tdt",
    "sb",
    "xt",
    "np"
)

for (i in 1:length(discrete.list)){
    print(discrete.list[[i]])
    print(table(raw.data[,1],raw.data[,discrete.list[[i]]]))
    cat("\n")
}
```

Compare Y/N responses across multiple variables and treatments:

```{r fig.width=15, fig.height=6}
#Y/N comparison across several attributes
plot.1 <- ggplot(raw.data, aes(x=wc, y=response, fill=response)) +
    geom_bar(stat = "identity", width = .5)
plot.2 <- ggplot(raw.data, aes(x=zwp, y=response, fill=response)) +
    geom_bar(stat = "identity", width = .5)
plot.3 <- ggplot(raw.data, aes(x=wi, y=response, fill=response)) +
    geom_bar(stat = "identity", width = .5)
plot.4 <- ggplot(raw.data, aes(x=dtj, y=response, fill=response)) +
    geom_bar(stat = "identity", width = .5)
plot.5 <- ggplot(raw.data, aes(x=bnf, y=response, fill=response)) +
    geom_bar(stat = "identity", width = .5)
plot.6 <- ggplot(raw.data, aes(x=ent, y=response, fill=response)) +
    geom_bar(stat = "identity", width = .5)
plot.7 <- ggplot(raw.data, aes(x=ypz, y=response, fill=response)) +
    geom_bar(stat = "identity", width = .5)
plot.8 <- ggplot(raw.data, aes(x=tdt, y=response, fill=response)) +
    geom_bar(stat = "identity", width = .5)
plot.9 <- ggplot(raw.data, aes(x=sb, y=response, fill=response)) +
    geom_bar(stat = "identity", width = .5)
plot.10 <- ggplot(raw.data, aes(x=ox, y=response, fill=response)) +
    geom_bar(stat = "identity", width = .5)
plot.11 <- ggplot(raw.data, aes(x=xt, y=response, fill=response)) +
    geom_bar(stat = "identity", width = .5)
plot.12 <- ggplot(raw.data, aes(x=np, y=response, fill=response)) +
    geom_bar(stat = "identity", width = .5)
plot.13 <- ggplot(raw.data, aes(x=ku, y=response, fill=response)) +
    geom_bar(stat = "identity", width = .5)
grid.arrange(plot.1, 
             plot.2, 
             plot.3, 
             plot.4, 
             plot.5, 
             plot.6,
             plot.7,
             plot.8,
             plot.9,
             plot.10,
             plot.11,
             plot.12,
             plot.13,
             ncol = 5)
```

The above charts are not that easy to read.  I will explicitly calculate the success rates for all categorical variables and plot them:

```{r}
# function to analyze the success rate by category
success.rate.by.level <- function(column){
    lvls <- levels(column)
    total <- length(column)
    counts <- numeric()
    rates <- numeric()
    Ycounts <- numeric()
    Yrates <- numeric()
    i <- 1
    for (l in 1:length(lvls)){
        counts[i] <- nrow(raw.data[column == lvls[l],])
        rates[i] <- 100 * (counts[i] / total)
        Ycounts[i] <- nrow(raw.data[column == lvls[l] & raw.data$response == "Y",])
        Yrates[i] <- 100 * (Ycounts[i] / counts[i])
        i <- i + 1
    }
    result <- as.data.frame(cbind(counts, rates, Ycounts, Yrates))
    result <- cbind(lvls, result)
    result <- result[order(Yrates),]
    result[,1] <- factor(result[,1], levels = result[,1])
    return(result)
}
```

```{r}
#calculate success rates
rates.wc <- success.rate.by.level(raw.data$wc)
rates.zwp <- success.rate.by.level(raw.data$zwp)
rates.wi <- success.rate.by.level(raw.data$wi)
rates.bnf <- success.rate.by.level(raw.data$bnf)
rates.ent <- success.rate.by.level(raw.data$ent)
rates.ypz <- success.rate.by.level(factor(raw.data$ypz))
rates.tdt <- success.rate.by.level(raw.data$tdt)
rates.sb <- success.rate.by.level(raw.data$sb)
rates.ox <- success.rate.by.level(raw.data$ox)
rates.xt <- success.rate.by.level(raw.data$xt)
rates.np <- success.rate.by.level(raw.data$np)
rates.ku <- success.rate.by.level(raw.data$ku)
rates.wc
rates.zwp
rates.wi
rates.bnf
rates.ypz
rates.tdt
rates.sb
rates.xt
rates.np
rates.ku
```

```{r}
# success rates for discrete variables plotted
plot.list <- vector(mode = "list", length = 9)
rates.list <- list(
    rates.wc,
    rates.zwp,
    rates.wi,
    rates.ypz,
    rates.tdt,
    rates.sb,
    rates.xt,
    rates.np
)

discrete.list <- list(
    "wc",
    "zwp",
    "wi",
    "ypz",
    "tdt",
    "sb",
    "xt",
    "np"
)

for (i in 1:8){
    tmp.name <- paste0(discrete.list[[i]])
    tmp.name <- ggplot(data = rates.list[[i]], aes(x=lvls, y=Yrates)) +
        geom_bar(data = rates.list[[i]], aes(x=lvls, y=rates), stat="identity", width = .5) +
        geom_point() +
        ggtitle(paste0(tmp.name, " success rate")) +
        ylab("Point=Success Rate; Bars=Occurance Rate") +
        geom_hline(yintercept = 5)
    plot.list[[i]] <- tmp.name
}

plot.list
```
Success rate plot and table conclusions:
wc: combine a/c/f; combine b/e; make 3 levels
zwp: combine S/R; combine T/O/U/N; combine V/X/W/Q/Y/P/Z; make 3 levels
wi: keep
bnf: evening combing O/N comprises less than 1% of the data; removing this variable
ypz: combine 1/4/11/0/6/14/7; combine 3/12/10; combine 15/19/16; combine 17/13/5/8/2; make 4 levels
tdt: keep
sb: combine s2/s5; combine the remaining; make 2 levels
xt: combine dgk/ntb/fcc; combine mrm/gcn; make 2 levels
np: keep

## Preprocessing

See the question 1 section "Exploratory Conclusions" for the rationale of each preprocessing operation.

Preprocessing operations:

```{r}
# make the id column the row names and remove it as a variable
pp.data.1 <- raw.data
rownames(pp.data.1) <- pp.data.1$id
pp.data.1$id <- NULL

# remove non-relevant variables
pp.data.1$ent <- NULL
pp.data.1$ox <- NULL
pp.data.1$bnf <- NULL

# wc: combine a/c/f; combine b/e; make 3 levels
pp.data.1$wc <- as.character(pp.data.1$wc)
pp.data.1$wc[pp.data.1$wc == "a" | pp.data.1$wc == "c" | pp.data.1$wc == "f"] <- "acf"
pp.data.1$wc[pp.data.1$wc != "acf" & pp.data.1$wc != "d"] <- "be"
pp.data.1$wc <- as.factor(pp.data.1$wc)

# zwp: combine S/R; combine T/O/U/N; combine V/X/W/Q/Y/P/Z; make 3 levels
pp.data.1$zwp <- as.character(pp.data.1$zwp)
pp.data.1$zwp[pp.data.1$zwp == "S" | pp.data.1$zwp == "R"] <- "SR"
pp.data.1$zwp[pp.data.1$zwp == "T" | pp.data.1$zwp == "O" | pp.data.1$zwp == "U" | pp.data.1$zwp == "N"] <- "TOUN"
pp.data.1$zwp[pp.data.1$zwp != "SR" & pp.data.1$zwp != "TOUN"] <- "VXWQYPZ"
pp.data.1$zwp <- as.factor(pp.data.1$zwp)

# ypz: combine 1/4/11/0/6/14/7; combine 3/12/10; combine 15/19/16; combine 17/13/5/8/2; make 4 levels
pp.data.1$ypz <- as.character(pp.data.1$ypz)
pp.data.1$ypz[pp.data.1$ypz == "3" | pp.data.1$ypz == "12"| pp.data.1$ypz == "10"] <- "3,12,10"
pp.data.1$ypz[pp.data.1$ypz == "15" | pp.data.1$ypz == "19"| pp.data.1$ypz == "16"] <- "15,19,16"
pp.data.1$ypz[pp.data.1$ypz == "17" | pp.data.1$ypz == "13"| pp.data.1$ypz == "5" | pp.data.1$ypz == "8"| pp.data.1$ypz == "2"] <- "17,13,5,8,2"
pp.data.1$ypz[pp.data.1$ypz != "3,12,10" & pp.data.1$ypz != "3,12,10" & pp.data.1$ypz != "17,13,5,8,2"] <- "1,4,11,0,6,14,7"
pp.data.1$ypz <- as.factor(pp.data.1$ypz)

#combine sb into s2/s5 and not s2/s5
pp.data.1$sb <- as.character(pp.data.1$sb)
pp.data.1$sb[(pp.data.1$sb == "s2") | (pp.data.1$sb == "s5")] <- "s2s5"
pp.data.1$sb[pp.data.1$sb != "s2s5"] <- "s1s3s4s6s7s8"
pp.data.1$sb <- as.factor(pp.data.1$sb)

# xt: combine dgk/ntb/fcc; combine mrm/gcn; make 2 levels
pp.data.1$xt <- as.character(pp.data.1$xt)
pp.data.1$xt[(pp.data.1$xt == "mrm") | (pp.data.1$xt == "gcn")] <- "mrm,gcn"
pp.data.1$xt[pp.data.1$xt != "mrm,gcn"] <- "dgk,ntb,fcc"
pp.data.1$xt <- as.factor(pp.data.1$xt)

# ku: create ordered factors to treat ku as continuous
pp.data.1$ku <- factor(pp.data.1$ku, levels = rates.ku[,1])
pp.data.1$ku <- as.numeric(pp.data.1$ku)

#rearrange to continuous and discrete
continuous.vec <- c("response", "dtj", "qh", "sci", "bw", "ku", "is")
discrete.vec <- c("response", unlist(discrete.list, use.names = FALSE))
colorder <- c(continuous.vec, discrete.vec[2:length(discrete.vec)])
pp.data.1 <- pp.data.1[,colorder]
head(pp.data.1)

#subset data for testing models
sampled.rows <- sample(seq(1, nrow(pp.data.1)),ceiling(0.2*nrow(pp.data.1)),replace = FALSE)
model.test.data <- pp.data.1[sampled.rows,]
dim(model.test.data)

#subset data for testing code
xs.sampled.rows <- sample(seq(1, nrow(pp.data.1)),ceiling(1/31*nrow(pp.data.1)),replace = FALSE)
xs.model.test.data <- pp.data.1[xs.sampled.rows,]
dim(xs.model.test.data)
```

The shape and summary of the preprocessed data:

```{r}
# understanding the shape of the new data frame
cat("Dims:", dim(pp.data.1))
cat("\n\n\n")
str(pp.data.1)
cat("\n\n\n")
summary(pp.data.1)
cat("\n\n\n")
#check for NA values
sapply(pp.data.1, function(x) sum(is.na(x)))
```

### Discrete Data - Processed

Contingency Table for Combined of Treatments of Categorical Variables:

```{r}
# discrete variable contingency tables
for (i in 8:15){
    print(names(pp.data.1)[i])
    print(table(pp.data.1[,1], pp.data.1[,i]))
    cat("\n")
}
```

A better way to visualize the data than the information in the contingency tables is below using the success rate function I created:

```{r}
#calculate success rates with reduced categories
rates.wc <- success.rate.by.level(pp.data.1$wc)
rates.zwp <- success.rate.by.level(pp.data.1$zwp)
rates.wi <- success.rate.by.level(pp.data.1$wi)
rates.ypz <- success.rate.by.level(factor(pp.data.1$ypz))
rates.tdt <- success.rate.by.level(pp.data.1$tdt)
rates.sb <- success.rate.by.level(pp.data.1$sb)
rates.xt <- success.rate.by.level(pp.data.1$xt)
rates.np <- success.rate.by.level(pp.data.1$np)
rates.wc
rates.zwp
rates.wi
rates.ypz
rates.tdt
rates.sb
rates.xt
rates.np
rates.list <- list(
    rates.wc,
    rates.zwp,
    rates.wi,
    rates.ypz,
    rates.tdt,
    rates.sb,
    rates.xt,
    rates.np
)
```

Replot the success rate data with the reduced number of treatments for categorical variables:

```{r}
# success rates for discrete variables plotted
plot.list <- vector(mode = "list", length = 8)

for (i in 1:length(plot.list)){
    tmp.name <- paste0(discrete.list[[i]])
    tmp.name <- ggplot(data = rates.list[[i]], aes(x=lvls, y=Yrates)) +
        geom_bar(data = rates.list[[i]], aes(x=lvls, y=rates), stat="identity", width = .5) +
        geom_point() +
        ggtitle(paste0(tmp.name, " success rate")) +
        ylab("Point=Success Rate; Bars=Occurance Rate") +
        geom_hline(yintercept = 5) +
        geom_hline(yintercept = 1)
    plot.list[[i]] <- tmp.name
}

plot.list
```

Consider dropping xt later due to low variance.  Examine using the fisher exact test later.

### Continuous Data - Processed

Histograms of the continuous data to see if they have a normal or near normal distribution:

```{r fig.width=15, fig.height=5}
hplt.1 <- ggplot(data = pp.data.1, aes(x = qh, color = response)) +
                geom_histogram() +
                ggtitle("qh")
hplt.2 <- ggplot(data = pp.data.1, aes(x = sci, color = response)) +
                geom_histogram() +
                ggtitle("sci")
hplt.3 <- ggplot(data = pp.data.1, aes(x = bw, color = response)) +
                geom_histogram() +
                ggtitle("bw")
hplt.4 <- ggplot(data = pp.data.1, aes(x = is, color = response)) +
                geom_histogram() +
                ggtitle("is")
hplt.5 <- ggplot(data = pp.data.1, aes(x = dtj, color = response)) +
                geom_histogram() +
                ggtitle("dtj")
hplt.6 <- ggplot(data = pp.data.1, aes(x = ku, color = response)) +
                geom_histogram() +
                ggtitle("ku")
grid.arrange(hplt.1, 
             hplt.2, 
             hplt.3, 
             hplt.4, 
             hplt.5, 
             hplt.6,
             ncol = 3)
```

qh, sci, and bw are somewhat normal.
is, dtj, and ku have shapes that can be transformed into near normal if required.  

Scatter plots and correlation coefficients for continuous data and response:

```{r fig.width=15, fig.height=5}
# pair plot of continuous data
ggpairs(pp.data.1[,1:7], 
        mapping = ggplot2::aes(colour=response), 
        progress = FALSE) +
    theme(legend.position = "bottom")
```

Larger boxplots of continuous data separated by response

```{r fig.width=15, fig.height=5}
bxplt.1 <- ggplot(data = pp.data.1, aes(x = response, y = dtj, fill = response)) +
                geom_boxplot(notch = TRUE) +
                ggtitle("dtj") +
                coord_flip()
bxplt.2 <- ggplot(data = pp.data.1, aes(x = response, y = qh, fill = response)) +
                geom_boxplot(notch = TRUE) +
                ggtitle("qh") +
                coord_flip()
bxplt.3 <- ggplot(data = pp.data.1, aes(x = response, y = sci, fill = response)) +
                geom_boxplot(notch = TRUE) +
                ggtitle("sci") +
                coord_flip()
bxplt.4 <- ggplot(data = pp.data.1, aes(x = response, y = bw, fill = response)) +
                geom_boxplot(notch = TRUE) +
                ggtitle("bw") +
                coord_flip()
bxplt.5 <- ggplot(data = pp.data.1, aes(x = response, y = ku, fill = response)) +
                geom_boxplot(notch = TRUE) +
                ggtitle("ku") +
                coord_flip()
bxplt.6 <- ggplot(data = pp.data.1, aes(x = response, y = is, fill = response)) +
                geom_boxplot(notch = TRUE) +
                ggtitle("is") +
                coord_flip()
grid.arrange(bxplt.1, 
             bxplt.2, 
             bxplt.3, 
             bxplt.4, 
             bxplt.5, 
             bxplt.6,
             ncol = 2)
```

dtj, qh, and bw have decent separation of their means between the Y and N responses.  These are good indicators.

ku, sci and is have some separation and may be of importance. 

### Scatter Plots of Continuous and Discrete Data

Selected scatter plots that show banding and clusting with the reponse and other categorical variables.

```{r fig.width=10, fig.height=15}
#there is a decent linear pattern present within qh
#clear clusters can be seen with the np variable
scat.1 <- ggplot(pp.data.1, aes(x=ku, y=qh, color = np)) +
                geom_point()
scat.2 <- ggplot(pp.data.1, aes(x=is, y=qh, color = np)) +
                geom_point()
scat.3 <- ggplot(pp.data.1, aes(x=bw, y=qh, color = np)) +
                geom_point()
scat.4 <- ggplot(pp.data.1, aes(x=sci, y=qh, color = np)) +
                geom_point()
scat.5 <- ggplot(pp.data.1, aes(x=dtj, y=qh, color = np)) +
                geom_point()

scat.6 <- ggplot(pp.data.1, aes(x=ku, y=qh, color = response)) +
                geom_point()
scat.7 <- ggplot(pp.data.1, aes(x=is, y=qh, color = response)) +
                geom_point()
scat.8 <- ggplot(pp.data.1, aes(x=bw, y=qh, color = response)) +
                geom_point()
scat.9 <- ggplot(pp.data.1, aes(x=sci, y=qh, color = response)) +
                geom_point()
scat.10 <- ggplot(pp.data.1, aes(x=dtj, y=qh, color = response)) +
                geom_point()

grid.arrange(scat.1, scat.6,
             scat.2, scat.7,
             scat.3, scat.8,
             scat.4, scat.9,
             scat.5, scat.10,
             ncol = 2)
```

For the np variable, treatment j is related to lower qh values and treatment k are higher qh values.  It is clear that qh and np are both highly important to predicting success.  

dtj and bw vs qh have the cleanest separation of clusters which again indicates a higher significance of dtj and bw with the highest being qh and np.  

Some banding can be seen in the is vs qh plots over several categorical variables:

```{r}
# banding can be seen in these plots
ggplot(pp.data.1, aes(x=is, y=qh, color = ypz)) +
    geom_point()
ggplot(pp.data.1, aes(x=is, y=qh, color = tdt)) +
    geom_point()
ggplot(pp.data.1, aes(x=sci, y=qh, color = zwp)) +
    geom_point()
```

## PCA

Preprocess Data for PCA:

```{r}
# isolate separate data for PCA
pca.data <- pp.data.1

#convert factor data to numeric
pca.data[discrete.vec] <- sapply(pca.data[discrete.vec],as.numeric)
head(pca.data)
```

Look for relationship betweens mean and variation to understand the importance of scaling:

```{r}
attrmeans <- apply(pca.data,2,mean)
attrvars <- apply(pca.data,2,var)
plot(attrmeans,attrvars,log="xy")
summary(attrmeans)
summary(attrvars)
```

Scaling is necessary due to the log relationship between mean and variance of the variables.

Create the PCA model and plot:

```{r}
#PCA model
pca <- prcomp(pca.data, scale. = TRUE, center = TRUE)
plot(pca)
```

PC1 vs PC2 plots:

```{r}
for (i in discrete.vec){
    plot(pca$x[,1:2], col=pca.data[,i], main = i)
}
```

PC2 vs PC3 plots:

```{r}
for (i in discrete.vec){
    plot(pca$x[,2:3], col=pca.data[,i], main = i)
}
```

PC3 vs PC4 plots:

```{r}
for (i in discrete.vec){
    plot(pca$x[,3:4], col=pca.data[,i], main = i)
}
```

## Univariate Analysis

### Continuous Variables

T-tests:

```{r}
for (i in 2:7){
    print(continuous.vec[i])
    print(
        t.test(
            pp.data.1[pp.data.1$response == "Y", continuous.vec[i]],
            pp.data.1[pp.data.1$response == "N", continuous.vec[i]])
    )
}
```

All variables are significant because their p-values are below 0.05.  is is the least significant but it still has a p-value of 0.0001594.  All others are highly significant.

### Discrete Variables

Fisher Exact Tests:

```{r}
for (i in discrete.vec[2:length(discrete.vec)]){
    print(i)
    print(
        fisher.test(
            table(pp.data.1$response, pp.data.1[,i]), 
            workspace = 10000000))
}
```

All variables have highly significant p-values.  Note that ent and ox were already removed from the data by visual inspection.  The below analysis confirms that decision because both of their p-values are above 0.01.

Double check variables previously eliminated were non-significant using the Fisher Exact Test:

```{r}
fisher.test(
        table(raw.data$response, raw.data$ent), 
        workspace = 100000)
fisher.test(
        table(raw.data$response, raw.data$ox), 
        workspace = 10000000)
fisher.test(
        table(raw.data$response, raw.data$bnf), 
        workspace = 10000000, simulate.p.value = TRUE)
```

Although the p-value for bnf is significant, comparitively the other categorical values are much more significant.  I removed this variable from consideration.

## Exploratory Conclusions

1) qh and np are the strongest continuous predictor based on the scatterplots are success rates.  Using qh plotted against any other continuous variable and coloring the points by np shows strong groupings that match fairly closely to the response variable.  Per the contingency table, using np by itself as a predictor has an accuracy over 70%.  

2) The following categorical variables had their number of levels reduced and combined.  This will revent rare occurance treatments from skewing the model.  The methodology was to target treatments with similar success rates that accounted for less than 5% of the number of observations and then combine those.  

wc: combine a/c/f; combine b/e; make 3 levels
zwp: combine S/R; combine T/O/U/N; combine V/X/W/Q/Y/P/Z; make 3 levels
ypz: combine 1/4/11/0/6/14/7; combine 3/12/10; combine 15/19/16; combine 17/13/5/8/2; make 4 levels
sb: combine sb into s2/s5 and not s2/s5; make 2 levels
xt: combine dgk/ntb/fcc; combine mrm/gcn; make 2 levels

3) Although ku was a factor type of data, I converted it to continuous because of the obvious linear pattern in the its success rate chart.  Linearity can also be seen in scatter plots of ku versus other continuous variables.  

4) ypz was an integer type of data however I converted it to a factor because of its bimodality and non-apparent linear treand in its histogram and pair plots.  The remaining numeric types of data were analyzed using histograms and pair plots to ensure normality and linearity.  

5) ent, bnf and ox were found to have no significant impact and/or a potential to skew the results on the response and therefore they were eliminated.  This was done visually and then confirmed with a fisher exact test.

6) After exploratory analysis and preprocessing I am moving forward with 6 continuous variables:

```{r}
continuous.vec[2:length(continuous.vec)]
summary(pp.data.1[,2:7])
```

...and 8 categorical variables:

```{r}
discrete.vec[2:length(discrete.vec)]
summary(pp.data.1[,8:15])
```

8) The PCA analysis required the data to be scaled and centered as well as the categorical values to be converted to numeric.  The scaling and center was required to accuratley compare variables on different scales and center points.  

7) Clearly the data cannot be predicted very well using a univariate model.  It is complex and needs to leverage several predictors.  


# Problem 2: logistic regression (20 points)

Develop logistic regression model of the outcome `response` as a function of multiple predictors in the model.  Which variables are significantly associated with the outcome?  Test model performance on multiple splits of data into training and test subsets and summarize it in terms of accuracy/error/sensitivity/specificity.

## Variable Selection

I am choosing lasso regression to evaluate the most impactful variables because ridge seemed to be overfitting significantly.  The ridge/lasso method will also allow me to use categorical values.

```{r}
# Perform lasso regression
lasso <- model.matrix(response~., pp.data.1)[,-1]
lasso.response <- as.numeric(pp.data.1$response)
lasso.reg <- glmnet(lasso, lasso.response, alpha = 1)
plot(lasso.reg)
```

```{r}
cvlassoRes <- cv.glmnet(lasso, lasso.response, alpha=1)
plot(cvlassoRes)
```

```{r}
log(cvlassoRes$lambda.1se)
cvlassoRes$lambda.1se
```

Using the 1 SE method reduces the possibility of overfitting.

```{r}
predict(lasso.reg,type="coefficients",s=cvlassoRes$lambda.1se)
sort(abs(predict(lasso.reg,type="coefficients",s=cvlassoRes$lambda.1se)[,1]), 
     decreasing = TRUE)
```

Here we see the most significant variables and treatments are (in rank order):
1) sb at s2/s5 level
2) qh
3) zwp at V/X/W/Q/Y/P/Z level
4) wi at gnq level
5) xt at mrm/gcn level
6) tdt at kv level
7) wc at b/e level
8) wc at d level
9) np at the k level
10) zwp at the T/O/U/N level
11) tdt at the zp level
12) ku
13) dtj
14) is

Although there are 14 coefficients they are derived from just 11 variables.  

## Drop non-selected variables

```{r}
select.data <- pp.data.1
select.data$sci <- NULL
select.data$bw <- NULL
select.data$ypz <- NULL
head(select.data)
```

## Logistic Regression

```{r}
glm.fit <- glm(response~., data=select.data, family=binomial)
summary(glm.fit)
```

The logistic regression model agrees with the lasso analysis about the relative importance of the variables. Certain levels of tdt are insignificant.  Other levels of that variable is however so it must stay in the model.

```{r}
sort(abs(glm.fit$coefficients[2:length(glm.fit$coefficients)]), decreasing = TRUE, index.return = TRUE)$x
```


Generate the predictions on the full data set:

```{r}
glm.predict <- predict(glm.fit, newdata=select.data, type = "response")
plot(glm.predict, col = ifelse(as.numeric(select.data$response) == 1, "blue", "red"))
abline(h=0.5)
```

Generate predictions on test/train splits:

```{r}
#specify number of samples
nrep <- 500
fence <- 0.5

#create storage
error.rate = numeric()
accuracy = numeric()
sensitivity = numeric()
specificity = numeric()

#sample loop
for ( i in 1:nrep){
    
    #split into test/train
    test.row.sample <- sample(seq(1,nrow(select.data)), 
                              ceiling(.3 * nrow(select.data)), 
                              replace = FALSE)
    test.split <- select.data[test.row.sample,]
    train.split <- select.data[-test.row.sample,]
    
    #fit and prdict the logistic regression model
    glm.fit <- glm(response~., data=train.split, family=binomial)
    glm.predict <- predict(glm.fit, newdata=test.split, type = "response")
    
    #prediction post processing
    glm.predict[glm.predict >= fence] <- 1 #force variable to 0/1
    glm.predict[glm.predict < fence] <- 0 #force variable to 0/1
    predicted <- glm.predict
    
    #compputing performance of the model
    truth <- as.numeric(test.split$response) - 1
    TP = sum(truth==1 & predicted==1)
    TN = sum(truth==0 & predicted==0)
    FP = sum(truth==0 & predicted==1)
    FN = sum(truth==1 & predicted==0)
    P = TP+FN 
    N = FP+TN
    error.rate[i] <- 100-(signif(sum(truth==predicted)*100/length(truth),3))
    accuracy[i] <- (signif(sum(truth==predicted)*100/length(truth),3))
    sensitivity[i] <- signif(100*TP/P,3)
    specificity[i] <- signif(100*TN/N,3)
}

logistic.performance <- cbind(error.rate, accuracy, sensitivity, specificity)
logistic.performance.mean <- as.data.frame(apply(logistic.performance, 2, mean))
colnames(logistic.performance.mean) <- paste0(nrep, "rep.logistic.mean")
logistic.performance.mean
```

## Extra points problem: interaction terms (5 extra points)

Assess the impact/significance of pairwise interaction terms for all pairwise combinations of covariates used in the model and report the top ten that most significantly improve model fit.

# Problem 3: linear discriminant analysis (15 points)

Fit linear discriminant analysis model of the outcome `response` as a function of the rest of covariates in the dataset.  Feel free to decide whether you want to use all of them or a subset of those.  Test resulting model performance on multiple splits of the data into training and test subsets, summarize it in terms of accuracy/error/sensitivity/specificity and compare them to those obtained for logistic regression.

```{r}
#test 
lda.fit <- lda(response~., data=select.data)
lda.predict <- predict(lda.fit, newdata = select.data[,2:ncol(select.data)])
predicted <- lda.predict$class
lda.fit
plot(lda.predict$posterior[,2], col = ifelse(as.numeric(select.data$response) == 1, "blue", "red"))
abline(h=0.5)
```

Variable influence on the model:

```{r}
abs(lda.fit$scaling)
sort(abs(lda.fit$scaling), decreasing = TRUE, index.return=TRUE)$x
```


```{r}
#specify number of samples
nrep <- 500

#create storage
error.rate = numeric()
accuracy = numeric()
sensitivity = numeric()
specificity = numeric()

#sample loop
for ( i in 1:nrep){
    
    #split into test/train
    test.row.sample <- sample(seq(1,nrow(select.data)), 
                              ceiling(.3 * nrow(select.data)), 
                              replace = FALSE)
    test.split <- select.data[test.row.sample,]
    train.split <- select.data[-test.row.sample,]
    
    #fit and prdict the lda regression model
    lda.fit <- lda(response~., data=train.split)
    lda.predict <- predict(lda.fit, newdata = test.split[,2:ncol(test.split)])
    predicted <- as.numeric(lda.predict$class) - 1
    
    #compputing performance of the model
    truth <- as.numeric(test.split$response) - 1
    TP = sum(truth==1 & predicted==1)
    TN = sum(truth==0 & predicted==0)
    FP = sum(truth==0 & predicted==1)
    FN = sum(truth==1 & predicted==0)
    P = TP+FN 
    N = FP+TN
    error.rate[i] <- 100-(signif(sum(truth==predicted)*100/length(truth),3))
    accuracy[i] <- (signif(sum(truth==predicted)*100/length(truth),3))
    sensitivity[i] <- signif(100*TP/P,3)
    specificity[i] <- signif(100*TN/N,3)
}

# results
lda.performance <- cbind(error.rate, accuracy, sensitivity, specificity)
lda.performance.mean <- as.data.frame(apply(lda.performance, 2, mean))
colnames(lda.performance.mean) <- paste0(nrep, "rep.lda.mean")
```

Compare LDA and Logistic Regression Performance:

```{r}
compare.table1 <- cbind(logistic.performance.mean, lda.performance.mean)
compare.table1
```

The accuracy is slightly lower for the LDA model than the logistic regression.  The LDA model has a slightly lower sensitivity but a slightly higher specificty.  

# Problem 4: random forest (15 points)

Develop random forest model of outcome `response`. Present variable importance plots and comment on relative importance of different attributes in the model.  Did attributes showing up as more important in random forest model also appear as significantly associated with the outcome by logistic regression?  Test model performance on multiple splits of data into training and test subsets, compare test and out-of-bag error estimates, summarize model performance in terms of accuracy/error/sensitivity/specificity and compare to the performance of logistic regression and LDA models above.

```{r}
rfRes <- randomForest(select.data[1:22000,2:ncol(select.data)],
                      select.data[1:22000,"response"],
                      mtry = 5, ntree = 1000, importance = TRUE)
rfTmpTbl <- table(select.data[22001:nrow(select.data),"response"],
                  predict(
                      rfRes,
                      newdata=select.data[22001:nrow(select.data),2:ncol(select.data)]))
rfTmpTbl
err=100*(1-sum(diag(rfTmpTbl))/sum(rfTmpTbl))
cat("Error Rate:",err)
cat("\nAccuracy:",100 - err,"\n")
print("Rank of Variable Importance")
sort(rfRes$importance[,3], decreasing = TRUE)
```

```{r}
varImpPlot(rfRes, main = "Variable Importance Plot")
```

mtry and ntrees were tuned.  Values of mtry used were 2-10.  The optimal value was selected as 5.  Values of ntrees used were 500-2000.  The optimal value was selected as 1000.

```{r}
# Fit random forest to train data, obtain test error:
rf.generator <- function( train.data, train.labels, test.data, test.labels,...){
  rfRes <- randomForest(train.data,train.labels,importance = TRUE, ...)
  rfPred <- predict(rfRes,newdata=test.data)
  rfTmpTbl <- table(test.labels,rfPred)
  #list of items to return from the function:
  ret.list <- list("model" = rfRes, "prediction.table" = rfTmpTbl, 
                   "predictions" = rfPred, "oob.err" = min(rfRes$err.rate[,1]),
                   "importance" = sort(rfRes$importance[,3], decreasing = TRUE))
  return(ret.list)
}

#specify the runs and hyperparameters
nrep <- 20
mtry.go <- 5
ntrees.go <- 1000

#create storage
error.rate = numeric()
oob.err.rate = numeric()
rf.importance <- NULL
accuracy = numeric()
sensitivity = numeric()
specificity = numeric()
importance.order <- names(select.data[,2:ncol(select.data)])

#sample loop
for ( i in 1:nrep){
    
    #split into test/train
    test.row.sample <- sample(seq(1,nrow(select.data)), 
                              ceiling(.3 * nrow(select.data)), 
                              replace = FALSE)
    test.split <- select.data[test.row.sample,]
    train.split <- select.data[-test.row.sample,]
    
    #fit and predict random forest model
    rf <- rf.generator(train.split[,2:ncol(train.split)], train.split$response,
                        test.split[,2:ncol(test.split)], test.split$response,
                       mtry = mtry.go,
                       ntree = ntrees.go)
    predicted <- as.numeric(rf$predictions) - 1

    #computing performance of the model
    truth <- as.numeric(test.split$response) - 1
    TP = sum(truth==1 & predicted==1)
    TN = sum(truth==0 & predicted==0)
    FP = sum(truth==0 & predicted==1)
    FN = sum(truth==1 & predicted==0)
    P = TP+FN 
    N = FP+TN
    oob.err.rate[i] <- rf$oob.err * 100
    rf.importance <- rbind(rf.importance, rf$importance[importance.order])
    error.rate[i] <- 100 * 
        (1-sum(diag(rf$prediction.table))/sum(rf$prediction.table))
    accuracy[i] <- 100 - error.rate[i]
    sensitivity[i] <- signif(100*TP/P,3)
    specificity[i] <- signif(100*TN/N,3)
}

# oob to test error results
rf.err.compare <- cbind(error.rate, oob.err.rate)
rf.err.compare.mean <- as.data.frame(apply(rf.err.compare, 2, mean))
colnames(rf.err.compare.mean) <- paste0(nrep, "rep.rf.mean")

# importance
rf.importance.mean <- as.data.frame(apply(rf.importance, 2, mean))
colnames(rf.importance.mean) <- paste0(nrep, "rep.rf.mean")

# model to model results
rf.performance <- cbind(error.rate, accuracy, sensitivity, specificity)
rf.performance.mean <- as.data.frame(apply(rf.performance, 2, mean))
colnames(rf.performance.mean) <- paste0(nrep, "rep.rf.mean")
```

```{r}
importance.labels <- c(
    rep(importance.order[1],nrep),
    rep(importance.order[2],nrep),
    rep(importance.order[3],nrep),
    rep(importance.order[4],nrep),
    rep(importance.order[5],nrep),
    rep(importance.order[6],nrep),
    rep(importance.order[7],nrep),
    rep(importance.order[8],nrep),
    rep(importance.order[9],nrep),
    rep(importance.order[10],nrep),
    rep(importance.order[11],nrep)
)

rf.melt.importance <- c(
    rf.importance[,1],
    rf.importance[,2],
    rf.importance[,3],
    rf.importance[,4],
    rf.importance[,5],
    rf.importance[,6],
    rf.importance[,7],
    rf.importance[,8],
    rf.importance[,9],
    rf.importance[,10],
    rf.importance[,11]
)

rf.melt.importance <- as.data.frame(cbind(importance.labels, rf.melt.importance))
colnames(rf.melt.importance) <- c("var","MeanDecreaseAccuracy")

ggplot(rf.melt.importance, aes(x=reorder(var,as.numeric(MeanDecreaseAccuracy), median), y=as.numeric(MeanDecreaseAccuracy))) +
    geom_boxplot() +
    ggtitle("Random Forest Variable Importance") +
    xlab("Variables") + ylab("Mean Decrease Accuracy") +
    coord_flip()
```

Compare to the logistic regression order of importance below:

```{r}
#logistic regression ranked order of variable importance
glm.order.importance <- as.data.frame(names(sort(abs(glm.fit$coefficients[2:length(glm.fit$coefficients)]), decreasing = TRUE, index.return = TRUE)$x))
colnames(glm.order.importance) <- "variable"
glm.order.importance
```

When the factor levels are removed from the list of logistic regression variables it becomes:

1) sb
2) qh
3) zwp
5) xt
4) wi
6) wc
7) tdt
8) np
9) ku
10) dtj
11) is

The first two variables match but are flip-flopped: qh and sb.  zwp also ranks in the top four of both models.  The least important variable "is" also matches as either the last or second to last.  The others vary in rank between the models but all are present in both models and are significant.

```{r}
rf.err.compare.mean

rf.melt.err <- c(rf.err.compare[,1],rf.err.compare[,2])
rf.err.labels <- c(
        rep("error.rate",nrep),
        rep("oob.err.rate",nrep))
 
rf.melt.err <- as.data.frame(cbind(rf.err.labels,rf.melt.err))
colnames(rf.melt.err) <- c("err.type", "err.rate")

ggplot(rf.melt.err, aes(x=err.type, y=as.numeric(as.character(err.rate)))) +
    geom_boxplot() +
    ggtitle("Random Forest OOB Error and Test Error") +
    ylab("Error Rate")
```

The mean oob error rate matches the mean test error reate very closely although the test error rate is slighly higher.  The OOB error rate is slighly lower but is still a good measure to prevent overfitting which is common in random forests.  It could be possible to use the OOB error in the future in lieu of test/train splits.

Compare the model's performance:

```{r}
compare.table2 <- cbind(compare.table1, rf.performance.mean)
compare.table2
```

The random forest outperforms the previous two models in accuracy by about 1 to 2%.  It has a significantly higher sensitivity than both as well.  I performs slightly lower than the other two in terms of specificity.  This is the best model so far.

```{r}
#pass results to the next rmd file through csv files
write.csv(lda.performance, "lda_performance.csv", row.names = FALSE)
write.csv(logistic.performance, "logistic_performance.csv", row.names = FALSE)
write.csv(rf.performance, "rf_performance.csv", row.names = FALSE)
write.csv(compare.table2, "compare_err_table.csv")
```

